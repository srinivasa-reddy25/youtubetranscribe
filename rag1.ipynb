{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# print(\"Loaded Environment Variables:\", dict(os.environ))\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# print(GOOGLE_API_KEY)\n",
    "\n",
    "\n",
    "YOUTUBE_VIDEO=\"https://www.youtube.com/watch?v=cdiD-9MMpb0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model=\"gemini-1.5-pro\"\n",
    "\n",
    "llm=ChatGoogleGenerativeAI(model=model, google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# llm=ChatOpenAI(model=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square root of 100 is 10.\n"
     ]
    }
   ],
   "source": [
    "result =llm.invoke(\"what is the square root of 100\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llm.invoke(\"what is 2+2?\")\n",
    "# print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Los Angeles Dodgers won the 2020 World Series, which was played during the COVID-19 pandemic.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser=StrOutputParser()\n",
    "\n",
    "chain = llm | parser\n",
    "\n",
    "chain.invoke(\"What MLB team Won the world series during the COVID-19 pandamic?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\n\\nAnswer the question based on the context below. If you cannot find the answer in the context,\\n just say \"I don\\'t know\". Don\\'t try to make up an answer.\\n\\n context: Mary\\'s sister is sussana.\\n\\n question: who is Mary\\'s sister\\'s ?\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "template=\"\"\"\n",
    "\n",
    "Answer the question based on the context below. If you cannot find the answer in the context,\n",
    " just say \"I don't know\". Don't try to make up an answer.\n",
    "\n",
    " context: {context}\n",
    "\n",
    " question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Mary's sister is sussana.\", question=\"who is Mary's sister's ?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mary'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain= prompt | llm | parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\":\"Mary's sister is sussana.\", \n",
    "    \"question\":\"who is sussana's sister's ?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINING CHAINS IT IS APART FROM THE FLOW \n",
    "\n",
    "we can combine different chains to create more complex workflows. for explample lets create a second chain that trans;ltes the result  from first chain into differenbt language \n",
    "\n",
    "![Alt Text](Screenshot 2025-03-11 at 23.09.25.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_prompt=ChatPromptTemplate.from_template(\n",
    "    \"translate {answer} to {language} \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Hindi word for \"one\" is **एक (ek)**.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "translate_chain=(\n",
    "    {\"answer\":chain, \"language\":itemgetter(\"Language\")}\n",
    "    | translate_prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "translate_chain.invoke(\n",
    "    {\n",
    "    \"context\": \"Mary's sister is Susana. She doesn't have any more siblings.\",\n",
    "    \"question\": \"How many sisters does mary have?\",\n",
    "    \"Language\": \"Hindi\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribing the youtube video \n",
    "\n",
    "the context we want to send to the model comes from a youtube video and transcribe it usning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import whisper\n",
    "from pytube import YouTube\n",
    "\n",
    "\n",
    "# Let's do this only if we haven't created the transcription file yet.\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "    youtube = YouTube(YOUTUBE_VIDEO)\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "    # Let's load the base model. This is not the most accurate\n",
    "    # model but it's fast.\n",
    "    whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        file = audio.download(output_path=tmpdir)\n",
    "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "\n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 Resource has been exhausted (e.g. check quota).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    chain.invoke({\n",
    "        \"context\": transcription,\n",
    "        \"question\": \"Is reading papers a good idea?\"\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think it's possible that physics has exploits and we should be trying to find them. arranging some\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "TextDocuments = loader.load()\n",
    "\n",
    "print(TextDocuments[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'transcription.txt'}, page_content=\"I think it's possible that physics has exploits and we should be trying to find them. arranging some\"), Document(metadata={'source': 'transcription.txt'}, page_content='arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow,'), Document(metadata={'source': 'transcription.txt'}, page_content='buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligences'), Document(metadata={'source': 'transcription.txt'}, page_content=\"intelligences are kind of like the next stage of development. And I don't know where it leads to.\"), Document(metadata={'source': 'transcription.txt'}, page_content='where it leads to. Like at some point, I suspect the universe is some kind of a puzzle. These')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(TextDocuments)\n",
    "print(docs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'transcription.txt'}, page_content=\"I think it's possible that physics has exploits and we should be trying to find them. arranging some kind of a crazy quantum mechanical system that somehow gives you buffer overflow, somehow gives you a rounding error in the floating point. Synthetic intelligences are kind of like the next stage of development. And I don't know where it leads to. Like at some point, I suspect the universe is some kind of a puzzle. These synthetic AIs will uncover that puzzle and solve it. The following is a conversation with Andre Kappathi, previously the director of AI at Tesla. And before that, at OpenAI and Stanford, he is one of the greatest scientist engineers and educators in the history of artificial intelligence. This is the Lex Friedman podcast to support it. Please check out our sponsors and now to your friends. Here's Andre Kappathi. What is a neural network? And what does it seem to do such a surprisingly good job of learning? What is a neural network? It's a mathematical abstraction of the\"), Document(metadata={'source': 'transcription.txt'}, page_content=\"abstraction of the brain. I would say that's how it was originally developed. At the end of the day, it's a mathematical expression. And it's a fairly simple mathematical expression when you get down to it. It's basically a sequence of meter smoke applies, whichever link dot products mathematically. And some non-linearities throw them in. And so it's a very simple mathematical expression. And it's got knobs in it. Many knobs. Many knobs. And these knobs are loosely related to basically the synapses in your brain. They're trainable, they're modifiable. And so the idea is we need to find the setting of the knobs that makes the neural net do whatever you want it to do, like classify images and so on. And so there's not too much mystery I would say in it. Like you might think that basically don't want to end out with too much meaning with respect to the brain and how it works. It's really just a complicated mathematical expression with knobs. And those knobs need a proper setting for it\"), Document(metadata={'source': 'transcription.txt'}, page_content=\"setting for it to do something desirable. Yeah, but poetry is just the collection of letters with spaces. But it can make us feel a certain way. And in that same way, when you get a large number of knobs together, whether it's inside the brain or inside a computer, they seem to they seem to surprise us with their power. Yeah, I think that's fair. So basically I'm underselling it by a lot because you definitely do get very surprising emergent behaviors out of these neural nets when they're large enough and trained on complicated enough problems. Like say, for example, the next word prediction in a massive data set from the internet. And then these neural nets take on pretty surprising magical properties. Yeah, I think it's kind of interesting how much you can get out of even very simple mathematical formalism. When your brain right now is talking, is it doing next word prediction? Or is it doing something more interesting? Well, it's definitely some kind of a generative model that's a\"), Document(metadata={'source': 'transcription.txt'}, page_content=\"model that's a GPT-like and prompted by you. Yeah, so you're giving me a prompt and I'm kind of like responding to it in a generative way. And by yourself, perhaps a little bit like are you adding extra prompts from your own memory inside your head? Or no? Well, definitely feels like you're referencing some kind of a declarative structure of like memory and so on. And then you're putting that together with your prompt and giving away some extra. How much of what you just said has been said by you before? Nothing, basically, right? No, but if you actually look at all the words you've ever said in your life and you do a search, you'll probably said a lot of the same words in the same order before. Yeah, could be. I mean, I'm using phrases that are common, etc. But I'm remixing it into a pretty sort of unique sentence at the end of the day. But you're right, definitely, there's like a ton of remixing. Why you didn't, you just like Magnus Carlson said, I'm rated 2,900, whatever, which is\"), Document(metadata={'source': 'transcription.txt'}, page_content=\"whatever, which is pretty decent. I think you're talking very, you're not giving enough credit to neural nets here. Why do they seem to, what's your best intuition about this emergent behavior? I mean, it's kind of interesting because I'm simultaneously underselling them. But I also feel like there's an element to which I'm over like, it's actually kind of incredible that you can get so much emergent magical behavior out of them, despite them being so simple mathematically. So I think those are kind of like two surprising statements that are kind of just juxtaposed together. And I think basically what it is is we are actually fairly good at optimizing these neural nets. And when you give them a hard enough problem, they are forced to learn very interesting solutions in the optimization. And those solutions basically have these emergent properties that are very interesting. There's wisdom and knowledge in the knobs. And so this representation that's in the knobs doesn't make sense to\")]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(TextDocuments)\n",
    "print(docs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of embedding: 768\n",
      "[0.05077454447746277, -0.058103397488594055, -0.01842026598751545, -0.025286737829446793]\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "embedde_query=embeddings.embed_query(\"Who os mary's sister?\")\n",
    "\n",
    "print(f\"length of embedding: {len(embedde_query)}\")\n",
    "\n",
    "print(embedde_query[:4])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# // for openai subsss \n",
    "\n",
    "\n",
    "\n",
    "# from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# embeddings = OpenAIEmbeddings() \n",
    "\n",
    "# embedde_query=embeddings.embed_query(\"What is the square root of 100\")\n",
    "\n",
    "# print(f\"length of embedding: {len(embedde_query)}\")\n",
    "# print(embedde_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedde_query2=embeddings.embed_query(\"dog is a animal\")\n",
    "\n",
    "sentence1=embeddings.embed_query(\"animal is not a dog\")\n",
    "sentence2=embeddings.embed_query(\"american and chinese are two different countries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.8425441147468032), np.float64(0.5983353623710814))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for checking the similarities \n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([embedde_query2], [sentence1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([embedde_query2], [sentence2])[0][0]\n",
    "\n",
    "query_sentence1_similarity, query_sentence2_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the vector store\n",
    "\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorStore1=DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"Mary's sister is Susana\",\n",
    "        \"John and Tommy are brothers\",\n",
    "        \"Patricia likes white cars\",\n",
    "        \"Pedro's mother is a teacher\",\n",
    "        \"Lucia drives an Audi\",\n",
    "        \"Mary has two siblings\",\n",
    "        \"Mercedes are amazing automobiles\"\n",
    "\n",
    "    ],\n",
    "    embedding=embeddings\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={}, page_content=\"Mary's sister is Susana\"),\n",
       "  np.float64(0.7797462256563394)),\n",
       " (Document(metadata={}, page_content='Mary has two siblings'),\n",
       "  np.float64(0.7638965117390767)),\n",
       " (Document(metadata={}, page_content='John and Tommy are brothers'),\n",
       "  np.float64(0.6143092531303924))]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorStore1.similarity_search_with_score(query=\"Who is Mary's sister?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content=\"Mary's sister is Susana\"),\n",
       " Document(metadata={}, page_content='Mary has two siblings'),\n",
       " Document(metadata={}, page_content='John and Tommy are brothers'),\n",
       " Document(metadata={}, page_content='Patricia likes white cars')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever1 = vectorStore1.as_retriever()\n",
    "retriever1.invoke(\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={}, page_content='Patricia likes white cars'),\n",
       "  Document(metadata={}, page_content='Lucia drives an Audi'),\n",
       "  Document(metadata={}, page_content=\"Pedro's mother is a teacher\"),\n",
       "  Document(metadata={}, page_content='Mercedes are amazing automobiles')],\n",
       " 'question': \"What color is Patricia's car?\"}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())\n",
    "setup.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=setup | prompt | llm | parser\n",
    "chain.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mercedes'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What'a a great car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will do for the actual transcription \n",
    "\n",
    "vectorStore2=DocArrayInMemorySearch.from_documents(docs, embedding=embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the context, AGI is mentioned in relation to intelligence, generative models, and world models, and it's something that might be achievable through digital interaction alone or possibly through interaction with the physical world via robotics like Optimus.  However, a precise definition of AGI is not provided.\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup2 = RunnableParallel(context=vectorStore2.as_retriever(), question=RunnablePassthrough())\n",
    "\n",
    "chain=setup2 | prompt | llm | parser\n",
    "\n",
    "chain.invoke(\"What is AGI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=(\n",
    "    { \"context\": vectorStore2.as_retriever(),\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"What is systhetic intelligence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srinivasareddymedam/Desktop/FirstRAG/.venv/lib/python3.13/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "#setting up the pinecone vector store \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = \"firstrag-underfitted\"\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    docs, embeddings, index_name=index_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='3453df23-0ad6-4371-bf9d-9769397755ce', metadata={'source': 'transcription.txt'}, page_content=\"It's like high quality audio and you're speaking usually pretty clearly. I don't know what open AI's plans are either. Yeah, there's always fun projects basically. And stable diffusion also is opening up a huge amount of experimentation. I would say in the visual realm and generating images and videos and movies. I'll think like videos now. And so that's going to be pretty crazy. That's going to almost certainly work and it's going to be really interesting when the cost of content creation is going to fall to zero. You used to need a painter for a few months to paint a thing and now it's going to be speak to your phone to get your video. So Hollywood will start using it to generate scenes, which completely opens up. Yeah, so you can make a movie like Avatar eventually for under a million dollars. Much less. Maybe just by talking to your phone. I mean, I know it sounds kind of crazy. And then there'd be some voting mechanism. Like how do you have a, like, would there be a show on\"),\n",
       "  0.63686043),\n",
       " (Document(id='c15f594f-4e28-44a0-b1fe-35d99615e9e7', metadata={'source': 'transcription.txt'}, page_content=\"build everything. Then we were building the features, so like hog features and things like that that detect these little statistical patterns from image patches. And then there was a little bit of learning on top of it, like a support vector machine or binary classifier for cat versus dog and images on top of the features. So we wrote the features, but we trained the last layer sort of the classifier. And then people are like, actually let's not even design the features because we can't honestly, we're not very good at it. So let's also learn the features. And then you end up with basically a convolution on your own where you're learning most of it. You're just specifying the architecture. And the architecture has tons of filling blanks, which is all the knobs. And you let the optimization write most of it. And so this transition is happening across the industry everywhere. And suddenly we end up with a ton of code that is written in neural net weights. And I was just pointing out\"),\n",
       "  0.614340127),\n",
       " (Document(id='7127783f-6c9c-4fb8-9b87-fdf64f48ecf6', metadata={'source': 'transcription.txt'}, page_content=\"get to another celebrity and might get into other big accounts. And then it'll just, so with just that simple goal, get them to respond. Yeah. Maximize the probability of actual response. Yeah, I mean, you could prompt a powerful model like this with their, it's opinion about how to do any possible thing you're interested in. So they will check us. They're kind of on track to become these oracles. I could sort of think of it that way. They are oracles currently is just text, but they will have calculators, they will have access to Google search, they will have all kinds of gadgets and gizmos, they will be able to operate the internet and find different information. And yeah, in some sense, that's kind of like currently what it looks like in terms of the development. Do you think it'll be an improvement eventually over what Google is for access to human knowledge? Like it'll be a more effective search engine to access human knowledge. I think there's definitely scope in building a\"),\n",
       "  0.60070771)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.similarity_search_with_score(query=\"What is Hollywood going to start doing?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hollywood will start using AI to generate scenes.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=(\n",
    "    { \"context\": pinecone.as_retriever(),\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is Hollywood going to start doing??\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
